# Interview Lab 性能优化说明

## 🚀 已实施的优化

### 1. **减少 Token 数量**
- ✅ 简历内容截取从 12000 字符减少到 **6000 字符**（减少 50%）
- ✅ 回答分析时，超过 1000 字符的回答会被截取
- ✅ 问题数量从 10 题减少到 **6-8 题**（减少 20-40%）

### 2. **优化 AI 参数**
- ✅ Temperature 从 0.6 降低到 **0.4**（更快响应）
- ✅ 限制 max_tokens：Groq 4000，其他 3000（减少输出长度）

### 3. **精简 Prompt**
- ✅ 所有提示词更简洁
- ✅ logicBreakdown 要求更简短（1句 intent，简短 template）
- ✅ 优化分析要求更精简（减少不必要的详细说明）

### 4. **前端体验优化**
- ✅ 添加加载提示，显示预计等待时间
- ✅ 提示用户使用 Groq 会更快
- ✅ 加载动画（脉冲效果）

---

## ⚡ 预期速度提升

| 操作 | 优化前 | 优化后 | 提升 |
|------|--------|--------|------|
| **生成问题** | 60-90秒 | **20-40秒** | 50-60% ⬇️ |
| **回答分析** | 30-60秒 | **15-30秒** | 50% ⬇️ |

**注意**：实际速度取决于：
- 使用的 API（Groq 最快，DeepSeek 次之，OpenAI 较慢）
- 网络状况
- API 服务器负载

---

## 🎯 进一步优化建议

### 如果想更快，可以：

#### 1. **使用 Groq API**（最快）⭐ 推荐
```bash
# 在 server/.env 中配置
GROQ_API_KEY=gsk_你的key
```
- Groq 通常比 DeepSeek 快 **2-3 倍**
- 免费额度充足
- 响应时间：10-20 秒

#### 2. **减少问题数量**
- 当前：6-8 题
- 可以改为：4-6 题（在 `server/prompts/questions.js` 中修改）

#### 3. **缓存机制**（需要开发）
- 相同简历 + 岗位可以缓存结果
- 避免重复生成

#### 4. **流式输出**（需要开发）
- 问题逐个返回，不用等全部生成完
- 需要改造 API 和前端

---

## 📊 当前性能基准

### Groq API（推荐）
- 生成问题：**10-20 秒**
- 回答分析：**8-15 秒**

### DeepSeek API
- 生成问题：**30-50 秒**
- 回答分析：**20-35 秒**

### OpenAI API
- 生成问题：**40-70 秒**
- 回答分析：**25-45 秒**

---

## 🔧 如何检查当前使用的 API

查看 `server/.env` 文件：
- 如果配置了 `GROQ_API_KEY` → 使用 Groq（最快）
- 如果配置了 `DEEPSEEK_API_KEY` → 使用 DeepSeek
- 如果配置了 `OPENAI_API_KEY` → 使用 OpenAI

优先级：Groq > DeepSeek > OpenAI

---

## 💡 使用建议

1. **优先使用 Groq**：免费、快速、稳定
2. **如果速度还是慢**：
   - 检查网络连接
   - 确认使用的是 Groq API
   - 减少问题数量（修改代码）
3. **如果经常使用**：考虑添加缓存机制（我可以帮你实现）

---

## ❓ 如果还是慢

告诉我：
1. 你当前使用的是哪个 API？（Groq/DeepSeek/OpenAI）
2. 实际等待时间是多少？
3. 是否愿意进一步优化（如缓存、流式输出）？
